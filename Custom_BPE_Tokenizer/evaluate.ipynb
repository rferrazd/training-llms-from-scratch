{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f29b0aa6",
   "metadata": {},
   "source": [
    "# Evaluate the Tokenizer\n",
    "\n",
    "Metrics:\n",
    "-> Final vocabulary size\n",
    "-> Tokenization consistency across splits\n",
    "-> Average tokens per sentence\n",
    "-> Compression ratio (original chars / tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "64a20fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "import tokenizers\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "from typing import List\n",
    "import statistics\n",
    "import re\n",
    "\n",
    "# Minimum sentence length in the val and test set\n",
    "MIN_LENGTH = 10\n",
    "\n",
    "# Load the my tokenizer\n",
    "tokenizer = Tokenizer.from_file(\"my_tokenier\")\n",
    "\n",
    "# # Create a HuggingFace wrapper around your tokenizer\n",
    "# hf_tokenizer = PreTrainedTokenizerFast(tokenizer_object=tokenizer)\n",
    "\n",
    "# # Save in HuggingFace format (creates multiple files)\n",
    "# hf_tokenizer.save_pretrained(\"my_hf_tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "26ac89d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['Hell', 'o ', 'world', '!']\n",
      "Encoded IDs: [9279, 304, 5811, 7]\n",
      "Decoded:  Hell o  world !\n"
     ]
    }
   ],
   "source": [
    "# Sanity check!\n",
    "enc = tokenizer.encode(\"Hello world!\")\n",
    "print(\"Tokens:\", enc.tokens)\n",
    "print(\"Encoded IDs:\", enc.ids)\n",
    "dec = tokenizer.decode(enc.ids)\n",
    "print(\"Decoded: \", dec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8d97c7d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset splits: dict_keys(['test', 'train', 'validation'])\n",
      "Validation set size: 3760\n",
      "Test set size: 4358\n"
     ]
    }
   ],
   "source": [
    "# Load the validation and test set\n",
    "dataset = load_dataset(\"Salesforce/wikitext\", \"wikitext-2-v1\")\n",
    "print(f\"Dataset splits: {dataset.keys()}\")\n",
    "print(f\"Validation set size: {len(dataset['validation'])}\")\n",
    "print(f\"Test set size: {len(dataset['test'])}\")\n",
    "validation_texts = dataset['validation']['text'] \n",
    "test_texts = dataset['test']['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3dff77ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply same preprocessing steps as that done to the training data# 2.3 Clean the Data\n",
    "\n",
    "# Remove duplicates\n",
    "val_data_clean = set(validation_texts)\n",
    "test_data_clean = set(test_texts)\n",
    "\n",
    "def clean_text(text):\n",
    "    # Remove <unk> tokens\n",
    "    text = re.sub(r'<unk>', '', text)\n",
    "    # Replace all sequences of whitespace (spaces, tabs, newlines) with a single space, and remove leading/trailing spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "# Remove <unk> tokens and normalize white spaces\n",
    "val_data_clean = [clean_text(text) for text in val_data_clean]\n",
    "test_data_clean = [clean_text(text) for text in test_data_clean]\n",
    "# Remove empty and very short sequences\n",
    "val_data_clean = [text for text in val_data_clean if len(text) > MIN_LENGTH]\n",
    "test_data_clean = [text for text in test_data_clean if len(text) > MIN_LENGTH]\n",
    "\n",
    "# Load clean train data \n",
    "with open(\"wikitext2_train_cleaned.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    train_data_clean = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "fae74a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded = tokenizer.encode(\"hit me baby one more time\")\n",
    "tokens = encoded.tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "27d58f70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding(num_tokens=7, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])\n",
      "Number of tokens: 7 7\n"
     ]
    }
   ],
   "source": [
    "print(encoded)\n",
    "print(\"Number of tokens:\", len(encoded), len(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd4e08c",
   "metadata": {},
   "source": [
    "**Final Vocab Size**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e3736e8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer vocab size:  30000\n"
     ]
    }
   ],
   "source": [
    "# Vocab Size\n",
    "print(\"Tokenizer vocab size: \" , tokenizer.get_vocab_size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5963f8b9",
   "metadata": {},
   "source": [
    "**Tokenization Consistency Across Splits**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7dadd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>split</th>\n",
       "      <th>total_sentences</th>\n",
       "      <th>total_tokens</th>\n",
       "      <th>avg_tokens_per_sentence</th>\n",
       "      <th>median_tokens_per_sentence</th>\n",
       "      <th>avg_token_length</th>\n",
       "      <th>median_token_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Train</td>\n",
       "      <td>21115</td>\n",
       "      <td>1648075</td>\n",
       "      <td>78.052332</td>\n",
       "      <td>72.0</td>\n",
       "      <td>6.268695</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Validation</td>\n",
       "      <td>2286</td>\n",
       "      <td>166765</td>\n",
       "      <td>72.950569</td>\n",
       "      <td>66.0</td>\n",
       "      <td>6.221275</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Test</td>\n",
       "      <td>2604</td>\n",
       "      <td>185057</td>\n",
       "      <td>71.066436</td>\n",
       "      <td>64.0</td>\n",
       "      <td>6.207855</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        split  total_sentences  total_tokens  avg_tokens_per_sentence  \\\n",
       "0       Train            21115       1648075                78.052332   \n",
       "1  Validation             2286        166765                72.950569   \n",
       "2        Test             2604        185057                71.066436   \n",
       "\n",
       "   median_tokens_per_sentence  avg_token_length  median_token_length  \n",
       "0                        72.0          6.268695                  7.0  \n",
       "1                        66.0          6.221275                  7.0  \n",
       "2                        64.0          6.207855                  7.0  "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_tokenization_stats(texts: List[str], tokenizer: tokenizers.Tokenizer, split_name: str):\n",
    "    \"\"\"Compute tokenization statistics for a given split\"\"\"\n",
    "    total_tokens = 0\n",
    "    total_chars = 0\n",
    "    total_sentences = 0\n",
    "    token_lengths = []\n",
    "    tokens_per_sentences = []\n",
    "    token_lengths_dict = {}\n",
    "    \n",
    "    for text in texts:\n",
    "        if text.strip():  # Skip empty texts\n",
    "            # Tokenize the text\n",
    "            encoded = tokenizer.encode(text)\n",
    "            tokens = encoded.tokens\n",
    "            \n",
    "            # Count statistics\n",
    "            total_tokens += len(tokens)\n",
    "            total_sentences += 1\n",
    "            token_lengths.extend([len(token) for token in tokens])\n",
    "            tokens_per_sentences.append(len(tokens))\n",
    "\n",
    "            # track token lengths:\n",
    "            for token in tokens:\n",
    "                if token not in token_lengths_dict:\n",
    "                    token_lengths_dict[token] = len(token)\n",
    "\n",
    "    \n",
    "    # Calculate averages\n",
    "    avg_tokens_per_sentence = total_tokens / total_sentences if total_sentences > 0 else 0\n",
    "    avg_token_length = sum(token_lengths) / len(token_lengths) if token_lengths else 0\n",
    "    \n",
    "    return {\n",
    "        'split': split_name,\n",
    "        'total_sentences': total_sentences,\n",
    "        'total_tokens': total_tokens,\n",
    "        'avg_tokens_per_sentence': avg_tokens_per_sentence,\n",
    "        'median_tokens_per_sentence': statistics.median(tokens_per_sentences),\n",
    "        'avg_token_length': avg_token_length,\n",
    "        'median_token_length': statistics.median(token_lengths_dict.values()),\n",
    "    }\n",
    "\n",
    "# Compute stats for all splits\n",
    "train_stats = compute_tokenization_stats(train_data_clean, tokenizer, \"Train\")\n",
    "val_stats = compute_tokenization_stats(val_data_clean, tokenizer, \"Validation\") \n",
    "test_stats = compute_tokenization_stats(test_data_clean, tokenizer, \"Test\")\n",
    "\n",
    "# Results\n",
    "stats_df = pd.DataFrame([train_stats, val_stats, test_stats])\n",
    "stats_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99152b5b",
   "metadata": {},
   "source": [
    "Notes about the tokenization consistency across splits metric:\n",
    "- All splits have identical median_token_length = 7.0\n",
    "- Stable average token length (all similar to median token length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc074f82",
   "metadata": {},
   "source": [
    "**Compression Ratio**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859c9a7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data size in bytes: 1150234\n",
      "Tokenized test data size in bytes: 185057\n"
     ]
    }
   ],
   "source": [
    "# ----- Original test data -----\n",
    "test_byte_size = sum(len(text.encode('utf-8')) for text in test_data_clean)\n",
    "print(f\"Test data size in bytes: {test_byte_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27c24ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Characters per token: 6.21\n",
      "Bytes per token: 6.22\n"
     ]
    }
   ],
   "source": [
    "def compute_compression_metrics(texts, tokenizer):\n",
    "    total_chars = 0\n",
    "    total_bytes = 0\n",
    "    total_tokens = 0\n",
    "    \n",
    "    for text in texts:\n",
    "        if text.strip():\n",
    "            encoded = tokenizer.encode(text)\n",
    "            total_chars += len(text)                   \n",
    "            total_bytes += len(text.encode('utf-8'))    # Memory usage, 1 character = 8 bites = 1 byte\n",
    "            total_tokens += len(encoded.tokens)         \n",
    "    \n",
    "    char_per_token = total_chars / total_tokens   # Characters per token\n",
    "    bytes_per_token = total_bytes / total_tokens  # Bytes per token\n",
    "    \n",
    "    return char_per_token, bytes_per_token\n",
    "\n",
    "char_per_token, bytes_per_token = compute_compression_metrics(test_data_clean, tokenizer)\n",
    "print(f\"Characters per token: {char_per_token:.2f}\")\n",
    "print(f\"Bytes per token: {bytes_per_token:.2f}\")\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c44415b",
   "metadata": {},
   "source": [
    "**Save Tokenizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "99bdaa25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer saved in HuggingFace format!\n"
     ]
    }
   ],
   "source": [
    "# Convert to HuggingFace format for better compatibility\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "# Create HuggingFace wrapper around your tokenizer\n",
    "hf_tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_object=tokenizer,\n",
    "    # Add special tokens mapping\n",
    "    pad_token=\"[PAD]\",\n",
    "    unk_token=\"[UNK]\",\n",
    "    cls_token=\"[CLS]\",\n",
    "    sep_token=\"[SEP]\",\n",
    "    mask_token=\"[MASK]\"\n",
    ")\n",
    "\n",
    "# Save in HuggingFace format (creates multiple files)\n",
    "hf_tokenizer.save_pretrained(\"my_bpe_tokenizer_hf\")\n",
    "print(\"Tokenizer saved in HuggingFace format!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c48bc0f",
   "metadata": {},
   "source": [
    "# Push to Hugging Face HUB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "e5ff4099",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repo 'Rogarcia18/wikitext2-bpe-tokenizer' already exists on Hugging Face Hub.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/Rogarcia18/wikitext2-bpe-tokenizer/commit/7cd0c069509785244550797bbebb6d8591a23fa2', commit_message='Add model card', commit_description='', oid='7cd0c069509785244550797bbebb6d8591a23fa2', pr_url=None, repo_url=RepoUrl('https://huggingface.co/Rogarcia18/wikitext2-bpe-tokenizer', endpoint='https://huggingface.co', repo_type='model', repo_id='Rogarcia18/wikitext2-bpe-tokenizer'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import HfApi, create_repo\n",
    "\n",
    "# 1 - Intialize the HF API\n",
    "api = HfApi()\n",
    "\n",
    "# 2 - Create the repository \n",
    "repo_id = \"Rogarcia18/wikitext2-bpe-tokenizer\"\n",
    "# Check if repo exists, only create if it does not exist\n",
    "try:\n",
    "    api.repo_info(repo_id=repo_id, repo_type=\"model\")\n",
    "    print(f\"Repo '{repo_id}' already exists on Hugging Face Hub.\")\n",
    "except Exception:\n",
    "    create_repo(\n",
    "        repo_id=repo_id,\n",
    "        repo_type=\"model\",\n",
    "        private=False # Set to True if you want it private\n",
    "    )\n",
    "    print(f\"Repo '{repo_id}' created on Hugging Face Hub.\")\n",
    "\n",
    "# 3 - Upload the tokenizer's files \n",
    "api.upload_folder(\n",
    "    folder_path=\"my_bpe_tokenizer_hf\",\n",
    "    repo_id=repo_id,\n",
    "    commit_message=\"Upload BPE tokenizer trained on WikiText-2\"\n",
    ")\n",
    "\n",
    "# 4 - Upload model card\n",
    "\n",
    "model_card_content = \"\"\"\n",
    "---\n",
    "license: apache-2.0\n",
    "tags:\n",
    "- tokenizer\n",
    "- bpe\n",
    "- wikitext2\n",
    "- nlp\n",
    "---\n",
    "\n",
    "# WikiText-2 BPE Tokenizer\n",
    "\n",
    "A Byte Pair Encoding (BPE) tokenizer trained on the WikiText-2 dataset.\n",
    "\n",
    "## Model Details\n",
    "- **Vocabulary Size**: 30,000 tokens\n",
    "- **Training Data**: WikiText-2 (Salesforce/wikitext)\n",
    "- **Special Tokens**: [PAD], [UNK], [CLS], [SEP], [MASK]\n",
    "- **Compression Ratio**: ~6.4 characters per token\n",
    "\n",
    "## Usage\n",
    "```python\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Rogarcia18/wikitext2-bpe-tokenizer\")\n",
    "```\n",
    "\n",
    "## Training Details\n",
    "- Dataset: WikiText-2 (wikitext-2-v1)\n",
    "- Preprocessing: Deduplication, <unk> removal, whitespace normalization, remove samples cases with less than 10 characters\n",
    "- Architecture: BPE with HuggingFace tokenizers library\n",
    "\"\"\"\n",
    "# Save model card\n",
    "with open(\"my_bpe_tokenizer_hf/README.md\", \"w\") as f:\n",
    "    f.write(model_card_content)\n",
    "\n",
    "api.upload_file(\n",
    "    path_or_fileobj=\"my_bpe_tokenizer_hf/README.md\",\n",
    "    path_in_repo=\"README.md\",\n",
    "    repo_id=repo_id,\n",
    "    commit_message=\"Add model card\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "6f2244ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try using the tokenizer\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Rogarcia18/wikitext2-bpe-tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "75501646",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4644, 330, 1309, 542, 909, 80, 314, 819, 293]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"This is my first tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d572ddce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
