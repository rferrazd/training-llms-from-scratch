{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbe9dcd6",
   "metadata": {},
   "source": [
    "# 1. Dataset Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "45d689ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset splits: dict_keys(['test', 'train', 'validation'])\n",
      "Train set size: 36718\n",
      "Validation set size: 3760\n",
      "Test set size: 4358\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import re\n",
    "# Load dataset\n",
    "dataset = load_dataset(\"Salesforce/wikitext\", \"wikitext-2-v1\")\n",
    "print(f\"Dataset splits: {dataset.keys()}\")\n",
    "print(f\"Train set size: {len(dataset['train'])}\")\n",
    "print(f\"Validation set size: {len(dataset['validation'])}\")\n",
    "print(f\"Test set size: {len(dataset['test'])}\")\n",
    "train_texts = dataset['train']['text']\n",
    "validation_texts = dataset['validation']['text'] \n",
    "test_texts = dataset['test']['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d57a8326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First non-empty sample at index 1:\n",
      "Text:  = Valkyria Chronicles III = \n",
      "...\n",
      "\n",
      "First non-empty sample at index 3:\n",
      "Text:  Senjō no Valkyria 3 : <unk> Chronicles ( Japanese : 戦場のヴァルキュリア3 , lit . Valkyria of the Battlefield 3 ) , commonly referred to as Valkyria Chronicles III outside Japan , is a tactical role @-@ playin...\n",
      "\n",
      "First non-empty sample at index 4:\n",
      "Text:  The game began development in 2010 , carrying over a large portion of the work done on Valkyria Chronicles II . While it retained the standard features of the series , it also underwent multiple adju...\n",
      "\n",
      "First non-empty sample at index 5:\n",
      "Text:  It met with positive sales in Japan , and was praised by both Japanese and western critics . After release , it received downloadable content , along with an expanded edition in November of that year...\n",
      "\n",
      "First non-empty sample at index 7:\n",
      "Text:  = = Gameplay = = \n",
      "...\n",
      "\n",
      "First non-empty sample at index 9:\n",
      "Text:  As with previous <unk> Chronicles games , Valkyria Chronicles III is a tactical role @-@ playing game where players take control of a military unit and take part in missions against enemy forces . St...\n"
     ]
    }
   ],
   "source": [
    "# Find first non-empty sample\n",
    "for i in range(10):\n",
    "    if train_texts[i].strip():  # Check if not empty after stripping whitespace\n",
    "        print(f\"\\nFirst non-empty sample at index {i}:\")\n",
    "        print(f\"Text: {train_texts[i][:200]}...\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e2badc",
   "metadata": {},
   "source": [
    "# 2. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed9cb98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original training samples: 36718\n",
      "After deduplication: 21338\n"
     ]
    }
   ],
   "source": [
    "# 2.1 Remove Duplicates\n",
    "unique_train = set(train_texts)\n",
    "\n",
    "print(f\"Original training samples: {len(train_texts)}\")\n",
    "print(f\"After deduplication: {len(unique_train)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cbcd143f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 unique training samples:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['',\n",
       " ' The \" off @-@ the @-@ cuff \" conclusion to production was one of the reasons the album was titled Loose . It was named partly after the spontaneous decisions she made when creating the album . The album is also called Loose because it is \" the opposite of calculated \" and came naturally to Furtado and Timbaland ; she called him her \" distant musical cousin because he was always pushing boundaries and always carving out his own path \" , which she believed she was doing with Loose . \" I think you have to keep surprising people as an artist , and I like that — I love doing that \" , she said . Loose was also named partly for the R & B girl group TLC , who Furtado said she <unk> for \" taking back their sexuality , showing they were complete women . \" She said she wanted the album to be \" assertive and cool \" and \" sexy but fun \" , like TLC , MC <unk> , Queen <unk> and Janet Jackson , who inspired Furtado because , as she put it , she was \" comfortable in her sexuality and womanhood \" when her 1993 single \" That \\'s the Way Love Goes \" was released . \\n',\n",
       " ' Many Arabic type fonts feature special ligatures for Allah . \\n',\n",
       " ' = Crash Boom Bang ! = \\n',\n",
       " \" The Assembly 's first meeting began with a sermon by William Twisse in the nave of Westminster Abbey on 1 July 1643 . The nave was so full that the House of Commons had to send members ahead to secure seats . Following the sermon , the divines processed to the Henry VII Chapel , which would be their place of meeting until 2 October when they moved to the warmer and more private Jerusalem Chamber . After their initial meeting they <unk> for about a week , as Parliament had not yet given specific instructions . \\n\"]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"First 5 unique training samples:\")\n",
    "list(unique_train)[:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eac4d627",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" \\nNOTES: \\n- It seems that each entry in the dataset is a text segment which is not necessairly structured. \\n- In this project we are 'training' a BPE tokenizer, which learns subword patterns from the given text data. \\n- Perhaps keeping the dataset in the format as it is is fine? But what if we were training a LLM, could we use each entry in the dataset as a training example?\\n\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" \n",
    "NOTES: \n",
    "- It seems that each entry in the dataset is a text segment which is not necessairly structured. \n",
    "- In this project we are 'training' a BPE tokenizer, which learns subword patterns from the given text data. \n",
    "- Perhaps keeping the dataset in the format as it is is fine? But what if we were training a LLM, could we use each entry in the dataset as a training example?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a3cc217b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of training data after removing short examples:  21271\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 2.2 Filter Empty Samples (and very short ones)\n",
    "MIN_LENGTH = 10 \n",
    "unique_train = list(unique_train)\n",
    "unique_train = [example for example in unique_train if len(example) > MIN_LENGTH]\n",
    "print(\"Length of training data after removing short examples: \", len(unique_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30378330",
   "metadata": {},
   "source": [
    "**Purpose of Removing <unk> Tokens**\n",
    "\n",
    "The <unk> token serves as a placeholder for unknown/out-of-vocabulary words in the original WikiText-2 dataset. Here's why we remove them:\n",
    "\n",
    "1. They're Not Real Words\n",
    "<unk> is a special marker, not actual text content\n",
    "It represents words that were replaced during the original dataset creation\n",
    "Including them would teach the tokenizer to treat <unk> as a meaningful word\n",
    "\n",
    "2. Tokenizer Training Contamination\n",
    "If we keep <unk> tokens, the BPE algorithm might learn to split them as \"<\", \"unk\", \">\"\n",
    "This creates artificial vocabulary entries that don't represent real language patterns\n",
    "The tokenizer should learn to handle unknown words naturally, not through explicit <unk> markers\n",
    "\n",
    "3. Real-World Usage\n",
    "In production, you want the tokenizer to break down unknown words into subwords\n",
    "Having <unk> in training teaches the model to use this placeholder instead of learning proper subword segmentation\n",
    "A well-trained BPE tokenizer should rarely need <unk> tokens\n",
    "\n",
    "4. Clean Learning Signal\n",
    "Removing <unk> forces the tokenizer to learn from actual text patterns\n",
    "The BPE algorithm will naturally create subword units that can handle new words\n",
    "This leads to better generalization to unseen vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c35f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.3 Clean the Data\n",
    "def clean_text(text):\n",
    "    # Remove <unk> tokens\n",
    "    text = re.sub(r'<unk>', '', text)\n",
    "    # Replace all sequences of whitespace (spaces, tabs, newlines) with a single space, and remove leading/trailing spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "unique_train = [clean_text(text) for text in unique_train]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "34a3d87c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final training samples: 21271\n",
      "Sample cleaned text: The \" off @-@ the @-@ cuff \" conclusion to production was one of the reasons the album was titled Loose . It was named partly after the spontaneous decisions she made when creating the album . The alb...\n",
      "Empty samples remaining: 1\n"
     ]
    }
   ],
   "source": [
    "# Verify cleaning worked\n",
    "print(f\"Final training samples: {len(unique_train)}\")\n",
    "print(f\"Sample cleaned text: {unique_train[0][:200]}...\")\n",
    "\n",
    "# Check for any remaining issues\n",
    "empty_count = sum(1 for text in unique_train if not text.strip())\n",
    "print(f\"Empty samples remaining: {empty_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e7182c24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty samples remaining: 0\n"
     ]
    }
   ],
   "source": [
    "# Remove again empty samples, we may have gotten one now do do removing <unk>\n",
    "unique_train = [text for text in unique_train if len(text)> MIN_LENGTH]\n",
    "empty_count = sum(1 for text in unique_train if not text.strip())\n",
    "print(f\"Empty samples remaining: {empty_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a3652860",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For training a BPE tokenizer, you should save your training set as a plain text file,\n",
    "# with one document (string) per line. This is the standard format expected by most\n",
    "# tokenizers, including Hugging Face's tokenizers library.\n",
    "with open(\"wikitext2_train_cleaned.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for doc in unique_train:\n",
    "        f.write(doc + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
