{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a752dfa1",
   "metadata": {},
   "source": [
    "# Understanding how a base tokenizer from hugginface works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6048b529",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/robertagarcia/Desktop/GenAI/Training LLMs from Scratch/training-llms-from-scratch/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "from transformers import AutoTokenizer, HfArgumentParser\n",
    "from transformers.models.gpt2.tokenization_gpt2 import bytes_to_unicode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4081383",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer class: GPT2TokenizerFast\n",
      "Name or path: bigcode/starcoder\n",
      "Vocab size: 49152\n",
      "Model max length: 1000000000000000019884624838656\n",
      "Is fast tokenizer: True\n",
      "Padding side: right\n",
      "Truncation side: right\n",
      "Special tokens: {'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'additional_special_tokens': ['<|endoftext|>', '<fim_prefix>', '<fim_middle>', '<fim_suffix>', '<fim_pad>', '<filename>', '<gh_stars>', '<issue_start>', '<issue_comment>', '<issue_closed>', '<jupyter_start>', '<jupyter_text>', '<jupyter_code>', '<jupyter_output>', '<empty_output>', '<commit_before>', '<commit_msg>', '<commit_after>', '<reponame>']}\n",
      "Added tokens: {0: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True), 1: AddedToken(\"<fim_prefix>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True), 2: AddedToken(\"<fim_middle>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True), 3: AddedToken(\"<fim_suffix>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True), 4: AddedToken(\"<fim_pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True), 5: AddedToken(\"<filename>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True), 6: AddedToken(\"<gh_stars>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True), 7: AddedToken(\"<issue_start>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True), 8: AddedToken(\"<issue_comment>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True), 9: AddedToken(\"<issue_closed>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True), 10: AddedToken(\"<jupyter_start>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True), 11: AddedToken(\"<jupyter_text>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True), 12: AddedToken(\"<jupyter_code>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True), 13: AddedToken(\"<jupyter_output>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True), 14: AddedToken(\"<empty_output>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True), 15: AddedToken(\"<commit_before>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True), 16: AddedToken(\"<commit_msg>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True), 17: AddedToken(\"<commit_after>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True), 18: AddedToken(\"<reponame>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True)}\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bigcode/starcoder\")\n",
    "print(\"Tokenizer class:\", type(tokenizer).__name__)\n",
    "print(\"Name or path:\", tokenizer.name_or_path)\n",
    "print(\"Vocab size:\", tokenizer.vocab_size)\n",
    "# Model max length is probably max input length\n",
    "print(\"Model max length:\", tokenizer.model_max_length)\n",
    "print(\"Is fast tokenizer:\", getattr(tokenizer, \"is_fast\", \"N/A\"))\n",
    "print(\"Padding side:\", tokenizer.padding_side)\n",
    "print(\"Truncation side:\", tokenizer.truncation_side)\n",
    "print(\"Special tokens:\", tokenizer.special_tokens_map)\n",
    "print(\"Added tokens:\", tokenizer.added_tokens_decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863d45f8",
   "metadata": {},
   "source": [
    "## **Encoding Example**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "93229a17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized text:\n",
      "tks1: [8257, 1098, 458, 526, 11830, 49]\n",
      "tks2: [77, 91, 326, 36, 436, 595, 328, 2155, 26, 35, 34, 2177]\n",
      "\n",
      "\n",
      "Decoded text: text1: How will this be encoded?\n",
      "text2: [i**2 for i in range(10)]\n"
     ]
    }
   ],
   "source": [
    "# Basic Text2Tokens\n",
    "\n",
    "text1 = \"How will this be encoded?\"\n",
    "tks1 = tokenizer.encode(text = text1)\n",
    "text2 = \"[i**2 for i in range(10)]\"\n",
    "tks2 = tokenizer.encode(text = text2)\n",
    "print(f\"Tokenized text:\\ntks1: {tks1}\\ntks2: {tks2}\")\n",
    "\n",
    "back1 = tokenizer.decode(tks1)\n",
    "back2 = tokenizer.decode(tks2)\n",
    "print(f\"\\n\\nDecoded text:\\ntext1: {text1}\\ntext2: {text2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "89d37599",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text1 tokens: ['How', 'Ġwill', 'Ġthis', 'Ġbe', 'Ġencoded', '?']\n",
      "Text2 tokens: ['[', 'i', '**', '2', 'Ġfor', 'Ġi', 'Ġin', 'Ġrange', '(', '1', '0', ')]']\n"
     ]
    }
   ],
   "source": [
    "# For the first text\n",
    "text1 = \"How will this be encoded?\"\n",
    "tokens1 = tokenizer.tokenize(text1)\n",
    "print(\"Text1 tokens:\", tokens1)\n",
    "\n",
    "# For the second text  \n",
    "text2 = \"[i**2 for i in range(10)]\"\n",
    "tokens2 = tokenizer.tokenize(text2)\n",
    "print(\"Text2 tokens:\", tokens2) # no spaces tokenized? \n",
    "\n",
    "# From cursor:\n",
    "# The Ġ symbol you see is a special character that this tokenizer uses to mark the beginning of words (it represents a space). So:\n",
    "# 'Ġwill' means \" will\" (space + will)\n",
    "# 'Ġthis' means \" this\" (space + this)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1ae2cb54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'How' -> 8257\n",
      "'Ġwill' -> 1098\n",
      "'Ġthis' -> 458\n",
      "'Ġbe' -> 526\n",
      "'Ġencoded' -> 11830\n",
      "'?' -> 49\n"
     ]
    }
   ],
   "source": [
    "# Show the mapping\n",
    "for i, token in enumerate(tokens1):\n",
    "    token_id = tokenizer.convert_tokens_to_ids([token])[0] # returns a list with a single element\n",
    "    print(f\"'{token}' -> {token_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1ac355ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*'] ....\n"
     ]
    }
   ],
   "source": [
    " # Basic letters, numbers, punctuation and etc.\n",
    " base_vocab = list(bytes_to_unicode().values())\n",
    " print(f\"{base_vocab[:10]} ....\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf112768",
   "metadata": {},
   "source": [
    "# Data Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dafe0acf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load dataset with streaming=True (reads one example at a time)\n",
    "dataset = load_dataset(\"smangrul/hug_stack\", split=\"train\", streaming=True)\n",
    "\n",
    "# Create an iterator (like a bookmark in a book)\n",
    "iter_dataset = iter(dataset)\n",
    "print(\"DATASET:\\n\")\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3011a589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First example text length: 2195\n"
     ]
    }
   ],
   "source": [
    "# Read one example at a time\n",
    "# The iterator itself gives you 1 data point per call to next()\n",
    "first_example = next(iter_dataset)\n",
    "# first_example.keys() --> dict_keys(['text', 'id', 'metadata', '__index_level_0__'])\n",
    "print(\"First example text length:\", len(first_example['text']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b1862682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 1: <!--Copyright 2023 The HuggingFace Team. All rights reserved.\n",
      "\n",
      "Licensed under the Apache License, Ve\n",
      "Batch 1: <!--Copyright 2023 The HuggingFace Team. All rights reserved.\n",
      "\n",
      "Licensed under the Apache License, Ve\n",
      "Batch 2: # Copyright 2022 The HuggingFace Team. All rights reserved.\n",
      "#\n",
      "# Licensed under the Apache License, V\n",
      "Batch 3: from .constants import (\n",
      "    MODEL_NAME,\n",
      "    OPTIMIZER_NAME,\n",
      "    RNG_STATE_NAME,\n",
      "    SAFE_MODEL_NAME\n"
     ]
    }
   ],
   "source": [
    "# Option 1: Use individual examples\n",
    "dataset = load_dataset(\"smangrul/hug_stack\", split=\"train\", streaming=True)\n",
    "iter_dataset = iter(dataset)\n",
    "\n",
    "example_1 = next(iter_dataset)\n",
    "example_2 = next(iter_dataset)\n",
    "example_3 = next(iter_dataset)\n",
    "print(\"Example 1:\", example_1['text'][:100])\n",
    "\n",
    "# Option 2: Use batch iterator (don't call next() first)\n",
    "dataset = load_dataset(\"smangrul/hug_stack\", split=\"train\", streaming=True)\n",
    "iter_dataset = iter(dataset)\n",
    "\n",
    "def batch_iterator(batch_size=3):\n",
    "    for i in range(0, 9, 3):\n",
    "        batch = []\n",
    "        for _ in range(3):\n",
    "            batch.append(next(iter_dataset)['text'])\n",
    "        yield batch\n",
    "\n",
    "for i, batch in enumerate(batch_iterator()):\n",
    "    print(f\"Batch {i+1}: {batch[0][:100]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069bb680",
   "metadata": {},
   "source": [
    "# Training a New Tokenizer\n",
    "\n",
    "Its main purpose is to create a custom tokenizer tailored to a specific language or domain (e.g., legal documents, medical texts), which often improves model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e6558592",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ORIGINAL GPT-2 TOKENIZER ===\n",
      "Text: def fibonacci(n):\n",
      "    if n <= 1:\n",
      "        return n\n",
      "    return fibonacci(n-1) + fibonacci(n-2)\n",
      "Tokens: ['def', 'Ġfib', 'onacci', '(', 'n', '):', 'ĊĠĠĠ', 'Ġif', 'Ġn', 'Ġ<=', 'Ġ', '1', ':', 'ĊĠĠĠĠĠĠĠ', 'Ġreturn', 'Ġn', 'ĊĠĠĠ', 'Ġreturn', 'Ġfib', 'onacci', '(', 'n', '-', '1', ')', 'Ġ+', 'Ġfib', 'onacci', '(', 'n', '-', '2', ')']\n",
      "Token IDs: [589, 28176, 34682, 26, 96, 711, 284, 415, 310, 2511, 225, 35, 44, 291, 442, 310, 284, 442, 28176, 34682, 26, 96, 31, 35, 27, 474, 28176, 34682, 26, 96, 31, 36, 27]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "=== NEW CODE-TRAINED TOKENIZER ===\n",
      "Text: def fibonacci(n):\n",
      "    if n <= 1:\n",
      "        return n\n",
      "    return fibonacci(n-1) + fibonacci(n-2)\n",
      "Tokens: ['def', 'Ġfibonacci', '(', 'n', '):', 'ĊĠĠĠ', 'Ġif', 'Ġn', 'Ġ<=', 'Ġ', '1', ':', 'ĊĠĠĠĠĠĠĠ', 'Ġreturn', 'Ġn', 'ĊĠĠĠ', 'Ġreturn', 'Ġfibonacci', '(', 'n', '-', '1', ')', 'Ġ+', 'Ġfibonacci', '(', 'n', '-', '2', ')']\n",
      "Token IDs: [293, 295, 26, 96, 279, 278, 362, 309, 353, 239, 35, 44, 313, 321, 309, 278, 321, 295, 26, 96, 31, 35, 27, 347, 295, 26, 96, 31, 36, 27]\n",
      "\n",
      "=== COMPARISON ===\n",
      "Original tokens: ['def', 'Ġfib', 'onacci', '(', 'n', '):', 'ĊĠĠĠ', 'Ġif', 'Ġn', 'Ġ<=', 'Ġ', '1', ':', 'ĊĠĠĠĠĠĠĠ', 'Ġreturn', 'Ġn', 'ĊĠĠĠ', 'Ġreturn', 'Ġfib', 'onacci', '(', 'n', '-', '1', ')', 'Ġ+', 'Ġfib', 'onacci', '(', 'n', '-', '2', ')']\n",
      "New tokens:     ['def', 'Ġfibonacci', '(', 'n', '):', 'ĊĠĠĠ', 'Ġif', 'Ġn', 'Ġ<=', 'Ġ', '1', ':', 'ĊĠĠĠĠĠĠĠ', 'Ġreturn', 'Ġn', 'ĊĠĠĠ', 'Ġreturn', 'Ġfibonacci', '(', 'n', '-', '1', ')', 'Ġ+', 'Ġfibonacci', '(', 'n', '-', '2', ')']\n",
      "Original IDs:   [589, 28176, 34682, 26, 96, 711, 284, 415, 310, 2511, 225, 35, 44, 291, 442, 310, 284, 442, 28176, 34682, 26, 96, 31, 35, 27, 474, 28176, 34682, 26, 96, 31, 36, 27]\n",
      "New IDs:        [293, 295, 26, 96, 279, 278, 362, 309, 353, 239, 35, 44, 313, 321, 309, 278, 321, 295, 26, 96, 31, 35, 27, 347, 295, 26, 96, 31, 36, 27]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers.models.gpt2.tokenization_gpt2 import bytes_to_unicode\n",
    "\n",
    "original_tokenizer = tokenizer\n",
    "\n",
    "# Sample code text\n",
    "code_text = \"def fibonacci(n):\\n    if n <= 1:\\n        return n\\n    return fibonacci(n-1) + fibonacci(n-2)\"\n",
    "\n",
    "print(\"=== ORIGINAL GPT-2 TOKENIZER ===\")\n",
    "print(\"Text:\", code_text)\n",
    "print(\"Tokens:\", original_tokenizer.tokenize(code_text))\n",
    "print(\"Token IDs:\", original_tokenizer.encode(code_text))\n",
    "print()\n",
    "\n",
    "# Train a new tokenizer on code data\n",
    "sample_code_texts = [\n",
    "    \"def fibonacci(n):\\n    if n <= 1:\\n        return n\\n    return fibonacci(n-1) + fibonacci(n-2)\",\n",
    "    \"class MyClass:\\n    def __init__(self):\\n        self.value = 0\",\n",
    "    \"import pandas as pd\\ndf = pd.read_csv('data.csv')\",\n",
    "    \"for i in range(10):\\n    print(i)\",\n",
    "    \"def hello():\\n    print('Hello, world!')\"\n",
    "]\n",
    "\n",
    "def code_batch_iterator():\n",
    "    for i in range(0, len(sample_code_texts), 2):\n",
    "        yield sample_code_texts[i:i+2]\n",
    "\n",
    "base_vocab = list(bytes_to_unicode().values())\n",
    "new_tokenizer = original_tokenizer.train_new_from_iterator(\n",
    "    code_batch_iterator(), \n",
    "    vocab_size=1000,  # Small vocab for demo\n",
    "    initial_alphabet=base_vocab\n",
    ")\n",
    "\n",
    "print(\"=== NEW CODE-TRAINED TOKENIZER ===\")\n",
    "print(\"Text:\", code_text)\n",
    "print(\"Tokens:\", new_tokenizer.tokenize(code_text))\n",
    "print(\"Token IDs:\", new_tokenizer.encode(code_text))\n",
    "print()\n",
    "\n",
    "# Compare the results\n",
    "print(\"=== COMPARISON ===\")\n",
    "print(\"Original tokens:\", original_tokenizer.tokenize(code_text))\n",
    "print(\"New tokens:    \", new_tokenizer.tokenize(code_text))\n",
    "print(\"Original IDs:  \", original_tokenizer.encode(code_text))\n",
    "print(\"New IDs:       \", new_tokenizer.encode(code_text))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
